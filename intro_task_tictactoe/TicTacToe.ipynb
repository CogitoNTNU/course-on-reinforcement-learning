{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f63b47a466f03969",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# TicTacToe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99586f2b-90d8-429f-b9c7-c0a61c335148",
   "metadata": {},
   "source": [
    "Vi skal nå se på hvordan vi kan bruke TD læring til å trene en agent til å spille TicTacToe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef40968-2905-4c7a-b720-7e0e7b9b527c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.getcwd())\n",
    "%pip install matplotlib numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b7abe90321a681",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-13T12:54:01.578086871Z",
     "start_time": "2024-02-13T12:54:01.305649271Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pickle\n",
    "from State import all_states\n",
    "from TicTacEnv import TicTacEnv\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9ac885-a2a9-4fed-af63-67ac8959b42c",
   "metadata": {},
   "source": [
    "Tic Tac Toe er ferdig implementert, men vi har lyst på mulighet til å spille selv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf3a4b4983078fe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-13T12:51:14.838593023Z",
     "start_time": "2024-02-13T12:51:14.835438283Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class HumanAgent:\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        self.symbol = None\n",
    "        self.keys = ['q', 'w', 'e', 'a', 's', 'd', 'z', 'x', 'c']\n",
    "        self.state = None\n",
    "\n",
    "    def reset(self):\n",
    "        pass\n",
    "\n",
    "    def set_state(self, state):\n",
    "        self.state = state\n",
    "\n",
    "    def set_symbol(self, symbol):\n",
    "        self.symbol = symbol\n",
    "\n",
    "    def act(self):\n",
    "        self.state.render()\n",
    "        key = input(\"Input your position:\")\n",
    "        data = self.keys.index(key)\n",
    "        i = data // 3\n",
    "        j = data % 3\n",
    "        return i, j, self.symbol"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6caf5c6230dc96e4",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Det er ofte også nyttig med mulighet for tilfeldige handlinger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b992a8cd60d9b77c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-13T12:51:15.753331985Z",
     "start_time": "2024-02-13T12:51:15.739431721Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def get_random_move(state, symbol):\n",
    "    next_states = []\n",
    "    next_positions = []\n",
    "    for i in range(3):\n",
    "        for j in range(3):\n",
    "            if state.board[i, j] == 0:\n",
    "                next_positions.append([i, j])\n",
    "                next_states.append(state.next_state(i, j, symbol).hash())\n",
    "    \n",
    "    action = next_positions[np.random.randint(len(next_positions))]\n",
    "    action.append(symbol)\n",
    "    return action\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38cb69cf-ba37-4814-b133-051ddd3d721e",
   "metadata": {},
   "source": [
    "Vi lager en agent som kan spille tilfeldig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761e2b55e1132c41",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-13T12:51:16.236496163Z",
     "start_time": "2024-02-13T12:51:16.233228967Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class RandomAgent:\n",
    "    def __init__(self, **kwargs):\n",
    "        self.symbol = None\n",
    "        self.state = None\n",
    "        \n",
    "    def reset(self):\n",
    "        pass\n",
    "\n",
    "    def set_state(self, state):\n",
    "        self.state = state\n",
    "\n",
    "    def set_symbol(self, symbol):\n",
    "        self.symbol = symbol\n",
    "\n",
    "    def act(self):\n",
    "        return get_random_move(self.state, self.symbol)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28f9271-9ae7-4995-bdf6-8e08f69d6708",
   "metadata": {},
   "source": [
    "Og mulighet til å spille mot tifleldig agenten som spiller tilfeldig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1b5f70ff0825a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-13T12:51:16.812713203Z",
     "start_time": "2024-02-13T12:51:16.806487106Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def play():\n",
    "    for i in range(2):\n",
    "        # Load agents\n",
    "        player1 = TODO\n",
    "        player2 = TODO\n",
    "        # Load environment\n",
    "        env = TicTacEnv(player1, player2)\n",
    "        # Play the game\n",
    "        winner = env.play()\n",
    "        if winner == player2.symbol:\n",
    "            print(\"You lose!\")\n",
    "        elif winner == player1.symbol:\n",
    "            print(\"You win!\")\n",
    "        else:\n",
    "            print(\"It is a tie!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a62745-52fa-4938-97d0-b2fcbd34e892",
   "metadata": {},
   "source": [
    "Show the random agent what you are made of!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f459d82caf7c37a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-13T12:51:33.846657521Z",
     "start_time": "2024-02-13T12:51:17.376174518Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "play()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9df1cb-ed00-4275-99b8-c3aa6fa91010",
   "metadata": {},
   "source": [
    "Lets play the random agent against itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8912d3b3875710a8",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-13T12:47:03.275755660Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def compete(turns):\n",
    "    player1 = RandomAgent()\n",
    "    player2 = RandomAgent()\n",
    "    env = TicTacEnv(player1, player2)\n",
    "    player1_win = 0.0\n",
    "    player2_win = 0.0\n",
    "    tie = 0.0\n",
    "    for _ in range(turns):\n",
    "        winner = env.play()\n",
    "        if winner == 1:\n",
    "            player1_win += 1\n",
    "        elif winner == -1:\n",
    "            player2_win += 1\n",
    "        else:\n",
    "            tie += 1\n",
    "        env.reset()\n",
    "    print('%d turns, player 1 win %.02f, player 2 win %.02f, tied %.02f'% (turns, player1_win / turns, player2_win / turns, tie / turns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfba8fb1-4045-4375-b278-07ef32ded4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "compete(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001c3acf-085a-4a48-b8cb-e53bf5f57cdf",
   "metadata": {},
   "source": [
    "How did it do? Was it what you would expect?\n",
    "What if you change the number of games?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e53a667-6241-45aa-bbf6-574d39d5e1f1",
   "metadata": {},
   "source": [
    "## RL agent\n",
    "\n",
    "This is where most of your code will go.\n",
    "\n",
    "Fill inn all the TODOs\n",
    "Feel free to ask questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdbbe75-3184-40a7-8920-56476306f10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "\n",
    "    def __init__(self, learning_rate=0.1, epsilon=0.1):\n",
    "        self.value_table = dict()\n",
    "        self.learning_rate = TODO\n",
    "        self.epsilon = TODO\n",
    "        self.states = []\n",
    "        self.greedy = []\n",
    "        self.symbol = 0\n",
    "\n",
    "    def reset(self):\n",
    "        # Reset the agents so its ready for a new episode\n",
    "        self.states = []\n",
    "        self.greedy = []\n",
    "\n",
    "    def set_state(self, state):\n",
    "        self.states.append(state)\n",
    "        self.greedy.append(True)\n",
    "\n",
    "    def set_symbol(self, symbol):\n",
    "        # Set the symbol the agent will use\n",
    "        self.symbol = symbol\n",
    "        for hash_val in all_states:\n",
    "            state, is_end = all_states[hash_val]\n",
    "            if is_end:\n",
    "                if state.winner == self.symbol:\n",
    "                    self.value_table[hash_val] = 1.0\n",
    "                elif state.winner == 0:\n",
    "                    # we need to distinguish between a tie and a lose\n",
    "                    self.value_table[hash_val] = 0.5\n",
    "                else:\n",
    "                    self.value_table[hash_val] = 0\n",
    "            else:\n",
    "                self.value_table[hash_val] = 0.5\n",
    "\n",
    "    def backup(self):\n",
    "        # Back up the values we have encountered\n",
    "        states = [state.hash() for state in self.states]\n",
    "\n",
    "        for i in reversed(range(len(states) - 1)):\n",
    "            # For each state we want to compute the TD error or \"Surprise\" of the system\n",
    "            state = TODO\n",
    "            td_error = TODO\n",
    "            self.value_table[state] += TODO\n",
    "\n",
    "    def act(self):\n",
    "        # Selecting an action for the agent to performe\n",
    "        state = self.states[-1]\n",
    "        next_states = []\n",
    "        next_positions = []\n",
    "        for i in range(3):\n",
    "            for j in range(3):\n",
    "                if state.board[i, j] == 0:\n",
    "                    next_positions.append([i, j])\n",
    "                    next_states.append(state.next_state(i, j, self.symbol).hash())\n",
    "\n",
    "        # If we have random less than epsilon we select an action randomly\n",
    "        if TODO:\n",
    "            action = next_positions[np.random.randint(len(next_positions))]\n",
    "            action.append(self.symbol)\n",
    "            self.greedy[-1] = False\n",
    "            return action\n",
    "\n",
    "        # Find our values for the next states\n",
    "        values = []\n",
    "        for hash_val, pos in zip(next_states, next_positions):\n",
    "            values.append((self.value_table[hash_val], pos))\n",
    "\n",
    "        # Lets sort all next states in decreasing value order\n",
    "        np.random.shuffle(values)\n",
    "        values.sort(key=lambda x: x[0], reverse=True)\n",
    "        action = values[0][1]\n",
    "        action.append(self.symbol)\n",
    "        # Return action that leads to the next state with highest value\n",
    "        return action\n",
    "\n",
    "    # It is very usefull to save our policy when we are done training\n",
    "    def save_policy(self):\n",
    "        with open('policy_%s.bin' % ('first' if self.symbol == 1 else 'second'), 'wb') as f:\n",
    "            pickle.dump(self.value_table, f)\n",
    "\n",
    "    # Likewise it is nice to be able to load our policy again\n",
    "    def load_policy(self):\n",
    "        with open('policy_%s.bin' % ('first' if self.symbol == 1 else 'second'), 'rb') as f:\n",
    "            self.value_table = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd033ea-d085-4c0a-8829-232676d0c16a",
   "metadata": {},
   "source": [
    "Now that we have the agent, we need to train it to play the game\n",
    "Again fill in the TODOs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e3bd26-6a91-415d-9f04-0f5ee3e096c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "\n",
    "    @staticmethod\n",
    "    def train(epochs, print_interval=500):\n",
    "        # Initialize the agents\n",
    "        p1 = TODO\n",
    "        p2 = TODO\n",
    "\n",
    "        # Initialize the environment\n",
    "        env = TODO\n",
    "        p1_wins = 0\n",
    "        p2_wins = 0\n",
    "\n",
    "        p1_100 = 0\n",
    "        p2_100 = 0\n",
    "        p1_history = []\n",
    "        p2_history = []\n",
    "        tie_history = []\n",
    "\n",
    "        # for each episode\n",
    "        for i in range(1, epochs + 1):\n",
    "            # Play a round and append the results\n",
    "            winner = TODO\n",
    "            if winner == 1:\n",
    "                p1_wins += 1\n",
    "                p1_100 += 1\n",
    "            if winner == -1:\n",
    "                p2_wins += 1\n",
    "                p2_100 += 1\n",
    "            # Print ocationaly\n",
    "            if i % print_interval == 0:\n",
    "                print('Epoch %d, player 1 winrate: %.02f, player 2 winrate: %.02f' % (\n",
    "                    i, p1_wins / i, p2_wins / i))\n",
    "            if i % 250 == 0:\n",
    "                p1_history.append(p1_100 / 250)\n",
    "                p2_history.append(p2_100 / 250)\n",
    "                tie_history.append((250 - p1_100 - p2_100) / 250)\n",
    "                p1_100 = 0\n",
    "                p2_100 = 0\n",
    "\n",
    "            # Train our agents\n",
    "            TODO\n",
    "            TODO\n",
    "\n",
    "            # Reset the envoronment\n",
    "            TODO\n",
    "\n",
    "        # Save the policies\n",
    "        TODO\n",
    "        TODO\n",
    "\n",
    "        # Plot the training hisory\n",
    "        plt.plot(p1_history, label=\"p1\")\n",
    "        plt.plot(p2_history, label=\"p2\")\n",
    "        plt.plot(tie_history, label=\"tie\")\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972edea6-4473-428d-8e57-534c702a4c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "Trainer.train(TODO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7258978f-181c-4468-905f-316438f97692",
   "metadata": {},
   "source": [
    "Lets test the two agents agains eachother"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7f4034-a335-4e4a-a4ed-fef3ddd289d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compete(turns):\n",
    "    player1 = TODO\n",
    "    player2 = TODO\n",
    "    env = TicTacEnv(player1, player2)\n",
    "    player1.load_policy()\n",
    "    player2.load_policy()\n",
    "    player1_win = 0.0\n",
    "    player2_win = 0.0\n",
    "    for _ in range(turns):\n",
    "        winner = env.play()\n",
    "        if winner == 1:\n",
    "            player1_win += 1\n",
    "        if winner == -1:\n",
    "            player2_win += 1\n",
    "        env.reset()\n",
    "    print('%d turns, player 1 win %.02f, player 2 win %.02f' % (turns, player1_win / turns, player2_win / turns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a32c4c-0d76-427f-a29b-689aa6521ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "compete(int(1e3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c1f345-d1b9-47f1-8a9d-5eddc5ed0fa9",
   "metadata": {},
   "source": [
    "How did it go? Did it play as expected?\n",
    "\n",
    "Play against it yourself!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101d898c-139c-4254-8191-13e8d018025d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play():\n",
    "    for i in range(2):\n",
    "        player1 = HumanAgent()\n",
    "        player2 = Agent(epsilon=0)\n",
    "        env = TicTacEnv(player1, player2)\n",
    "        player2.load_policy()\n",
    "        winner = env.play()\n",
    "        if winner == player2.symbol:\n",
    "            print(\"You lose!\")\n",
    "        elif winner == player1.symbol:\n",
    "            print(\"You win!\")\n",
    "        else:\n",
    "            print(\"It is a tie!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0d1bee-c724-4a1e-ae75-9d64f4ed98a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "play()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7858f39-8603-4647-8e2a-caae300ad461",
   "metadata": {},
   "source": [
    "Done?\n",
    "\n",
    "Try to train if for longer, shorter."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
